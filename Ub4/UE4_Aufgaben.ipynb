{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uebung 4 Mehrklassenklassifikation mit Neuronalen Netzen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from skimage.transform import rescale, resize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as scp \n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, widgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globale Variablen\n",
    "Definiere an dieser Stelle alle Variablen, die global verwendet werden, z.B.: Pfadnamen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTrafficSigns(rootpath, classes=None, resize_shape=None):\n",
    "    \"\"\"\n",
    "    Reads traffic sign data for German Traffic Sign Recognition Benchmark.\n",
    "\n",
    "    Arguments: path to the traffic sign data, for example './GTSRB/Training'\n",
    "            classes intended to be read\n",
    "    Returns:   list of images, list of corresponding labels\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # for c in range(0,43):\n",
    "\n",
    "    if classes is None:\n",
    "        # loop over all 42 classes\n",
    "        classes = [i for i in range(0,43)]\n",
    "    \n",
    "    for c in classes:\n",
    "        # subdirectory for class\n",
    "        prefix = rootpath + '/' + format(c, '05d') + '/'\n",
    "        \n",
    "        # annotations file\n",
    "        gtFile = open(prefix + 'GT-'+ format(c, '05d') + '.csv')\n",
    "        \n",
    "        # csv parser for annotations file\n",
    "        gtReader = csv.reader(gtFile, delimiter=';')\n",
    "        \n",
    "        # Skip header\n",
    "        next(gtReader)\n",
    "        \n",
    "        # loop over all images in current annotations file\n",
    "        for row in gtReader:\n",
    "            # the 1th column is the filename\n",
    "            img = plt.imread(prefix + row[0])\n",
    "            if resize_shape is not None:\n",
    "                    img = tf.image.convert_image_dtype(img, tf.float32) # equivalent to dividing image pixels by 255\n",
    "                    img = tf.image.resize(img, resize_shape) # Resizing the image to 224x224 dimention\n",
    "            \n",
    "            images.append(img)\n",
    "            \n",
    "            # the 8th column is the label\n",
    "            labels.append(int(row[7]))\n",
    "            \n",
    "        gtFile.close()\n",
    "    \n",
    "    # Convert list of labels to array of labels\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def getImagesByClassID(all_images, all_labels, class_id):\n",
    "    \"\"\"\n",
    "    Gets images by class id from the list of all images\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get index for the elements that match with the specified class id\n",
    "    class_id_idx = np.where(all_labels == class_id)[0]\n",
    "    \n",
    "    # Get images and labels for the two classes\n",
    "    labels = list(all_labels)[class_id_idx.min() : class_id_idx.max()]\n",
    "    images = all_images[class_id_idx.min() : class_id_idx.max()]\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def randomlyMixDatasets(data_sorted, labels_sorted):\n",
    "    \"\"\"\n",
    "    Randomly mixes sorted data and label lists in the same way\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    combined_set = list(zip(data_sorted, labels_sorted))\n",
    "    random.shuffle(combined_set)\n",
    "    data, labels = zip(*combined_set)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def onehotencoderlabels(labels):\n",
    "    b = np.zeros((labels.size, labels.max()+1))\n",
    "    b[np.arange(labels.size),labels] = 1\n",
    "    idx = np.argwhere(np.all(b[..., :] == 0, axis=0))\n",
    "    b = np.delete(b, idx, axis=1)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00001', '00007']\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = f\"{os.environ['HOME']}/Documents/TU Berlin/10 SS22/BGA II/UBs/data\"\n",
    "cl = [random.randint(0, 42) for _ in range(2)]\n",
    "CLASSES = [str('%05d' % i) for i in cl]\n",
    "images_folders = ['GTSRB_Final_Training_Images/GTSRB/Final_Training/Images',\n",
    "    'GTSRB_Final_Test_Images/GTSRB/Final_Test/Images', # + Classes Folder\n",
    "    ]\n",
    "hog_paths = ['GTSRB_Final_Test_HOG/GTSRB/Final_Test/HOG/HOG_01',\n",
    "    'GTSRB_Final_Test_HOG/GTSRB/Final_Test/HOG/HOG_02',\n",
    "    'GTSRB_Final_Test_HOG/GTSRB/Final_Test/HOG/HOG_03',\n",
    "    'GTSRB_Final_Training_HOG/GTSRB/Final_Training/HOG/HOG_01', # + Classes Folder\n",
    "    'GTSRB_Final_Training_HOG/GTSRB/Final_Training/HOG/HOG_02', # + Classes Folder\n",
    "    'GTSRB_Final_Training_HOG/GTSRB/Final_Training/HOG/HOG_03', # + Classes Folder\n",
    "    ]\n",
    "TRAIN_PATH = os.path.join(DATA_PATH, \"GTSRB_Final_Training_Images/GTSRB/Final_Training/Images\")\n",
    "TEST_PATH = os.path.join(DATA_PATH, \"GTSRB_Final_Test_Images/GTSRB/Final_Test/Images\")\n",
    "print(CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datenaufbereitung\n",
    "Hinweise findest du hier: https://keras.io/getting_started/intro_to_keras_for_engineers/#data-loading-amp-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-23 16:08:16.014286: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-23 16:08:16.091160: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-23 16:08:16.174000: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_images) = 3660\n",
      "all_labels.shape = (3660,)\n"
     ]
    }
   ],
   "source": [
    "# Read all the Traffic Sign data\n",
    "all_images, all_labels = readTrafficSigns(TRAIN_PATH, cl, (30, 30))\n",
    "\n",
    "print(\"len(all_images) = {}\".format(len(all_images)))\n",
    "print(\"all_labels.shape = {}\".format(all_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3660, 30, 30, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_images = np.array(all_images)\n",
    "all_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_labels.shape = (3660, 2)\n"
     ]
    }
   ],
   "source": [
    "all_labels = onehotencoderlabels(all_labels)\n",
    "print(\"all_labels.shape = {}\".format(all_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(all_images[0]) = <class 'numpy.ndarray'>\n",
      "all_images[0].shape = (30, 30, 3)\n",
      "[30]\n",
      "[30]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print(\"type(all_images[0]) = {}\".format(type(all_labels)))\n",
    "print(\"all_images[0].shape = {}\".format(all_images[0].shape))\n",
    "\n",
    "count = 0\n",
    "img_shape_0 = []\n",
    "img_shape_1 = []\n",
    "img_shape_2 = []\n",
    "for i,a in enumerate(all_images):\n",
    "  img_shape_0.append(a.shape[0])\n",
    "  img_shape_1.append(a.shape[1])\n",
    "  img_shape_2.append(a.shape[2])\n",
    "\n",
    "print(np.unique(img_shape_0))\n",
    "print(np.unique(img_shape_1))\n",
    "print(np.unique(img_shape_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45582bea6f074e45900a454f59e3c835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='idx', max=1170), Output()), _dom_classes=('widget-interaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_img(idx):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(all_images[idx])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "interact(show_img, idx=widgets.IntSlider(min=0,max=len(all_images), step=1, value=0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufbau des Modells\n",
    "Zum Aufbau deines Modells kannst du dich an die gezeigten Beispiele richten. Implementiere zuerst ein einfaches Modell, welches du je nach Performance erweitern kannst. \n",
    "\n",
    "Unten findest du die Auflistung der Schichten (Layers), die du fÃ¼r dein Modell miteinander kombinieren kannst. \n",
    "\n",
    "\n",
    "Ãœberlege dir, welche Layers fÃ¼r die Klassifikationsaufgabe mit HOG-Features gut sind und welche Layer sich fÃ¼r die Klassifikationsaufgabe mit ppm-Dateien eignen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (30, 30, 3)\n",
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=input_shape))\n",
    "model.add(keras.layers.Conv2D(20, 20, activation='relu'))\n",
    "model.add(keras.layers.Conv2D(5, 5, activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(2))\n",
    "model.add(keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kompilieren des Modells\n",
    "Eine detaillierte Beschreibung der [compile](https://keras.io/api/models/model_training_apis/#compile-method)-Methode findest du in Keras API Referenz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "            #   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                       tf.keras.metrics.FalseNegatives()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training des Modells\n",
    "Eine detaillierte Beschreibung der [fit](https://keras.io/api/models/model_training_apis/#fit-method)-Methode findest du in Keras API Referenz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_images, all_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "86/86 [==============================] - 10s 116ms/step - loss: 0.3634 - binary_accuracy: 0.8725 - false_negatives_1: 350.0000\n",
      "Epoch 2/5\n",
      "86/86 [==============================] - 8s 95ms/step - loss: 0.2496 - binary_accuracy: 0.9290 - false_negatives_1: 195.0000\n",
      "Epoch 3/5\n",
      "86/86 [==============================] - 8s 91ms/step - loss: 0.1922 - binary_accuracy: 0.9468 - false_negatives_1: 146.0000\n",
      "Epoch 4/5\n",
      "86/86 [==============================] - 9s 106ms/step - loss: 0.1359 - binary_accuracy: 0.9650 - false_negatives_1: 96.0000\n",
      "Epoch 5/5\n",
      "86/86 [==============================] - 9s 109ms/step - loss: 0.1306 - binary_accuracy: 0.9599 - false_negatives_1: 110.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f483c187130>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation des Modells\n",
    "Eine detaillierte Beschreibung der [evaluate](https://keras.io/api/models/model_training_apis/#evaluate-method)-Methode findest du in Keras API Referenz.\n",
    "\n",
    "Nach der Anwendung der *evaluate*-Methode kannst du dir zusÃ¤tzlich den ausfuehrlichen Klassifikationsbericht (*classification_report()*) sowie die Konfusionsmatrix (*confusion_matrix()*) anschauen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 1s 23ms/step - loss: 0.1094 - binary_accuracy: 0.9749 - false_negatives_1: 23.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.10940083861351013, 0.9748634099960327, 23.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speichern des trainierten Modells\n",
    "Zum Speichern des trainierten Modells kann *save*-Methode\n",
    "Weiterfuehrende Informationen zu dieser Methode unter folgendem [Link](https://keras.io/api/models/model_saving_apis/) zu finden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nutzen des trainierten Modells zum Vorhersagen von Verkehrszeichen-Klassen \n",
    "Zum Wiederverwenden des trainierten gespeicherten Modells kann die [load_model](https://keras.io/api/models/model_saving_apis/#loadmodel-function)-Funktionverwendet werden.\n",
    "\n",
    "Eine detaillierte Beschreibung der [predict](https://keras.io/api/models/model_training_apis/#predict-method)-Methode findest du in Keras API Referenz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 11, 11, 20)        24020     \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 5)           2505      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 3, 3, 5)           0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 3, 5)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 45)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 92        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 26,617\n",
      "Trainable params: 26,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_model = keras.models.load_model(\"./model.h5\")\n",
    "tmp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "252eba9a971ead2f09c02d843b707c0117a250435484e87bac08005f61cbe421"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('BGA2_SoSe22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
